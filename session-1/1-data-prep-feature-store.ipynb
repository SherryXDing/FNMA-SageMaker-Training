{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fannie Mae: Amazon SageMaker Training Series – Session 1\n",
    "\n",
    "## Hands-on Lab: Run SageMaker Processing to process your data and ingest features into Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [PART I - Processing our dataset locally](##Processing-our-dataset-locally)\n",
    "- [PART II -SageMaker Processing](##SageMaker-Processing)\n",
    "- [PART III - SageMaker Feature Store](##SageMaker-Feature-Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first notebook which will explore the data preparation stage of the ML workflow. Please select `Data Science 2.0` kernel image, instance type `ml.t3.medium` to run this notebook.\n",
    "\n",
    "Here, we will put on the hat of `Data Scientist`/`Data Engineer` and will perform the tasks of gathering datasets, pre-processing those datasets to align with our upcoming Training needs.  As part of this exercise, we will start by performing these steps manually inside our Notebook local environment. Then we will learn how to bring scale these steps using managed SageMaker processing capabilities. In the last step, we we will save the outcomes of our data processing to a SageMaker Feature Store.\n",
    "\n",
    "![Notebook1](./images/notebook-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable you to run these notebooks within a reasonable time (typically less than an hour), the use case is a straightforward regression task:  predicting house prices based on a synthetic housing dataset. This dataset contains 8 housing features. Features include year built, number of bedrooms, size of lot, etc...\n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for local training and test data. We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output. The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.171.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.153)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.24.3)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.5.3)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.153 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.29.153)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from jsonschema->sagemaker) (67.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.153->boto3<2.0,>=1.26.131->sagemaker) (1.26.16)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from time import strftime\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import time\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_data_path = \"./data/raw/house_pricing.csv\"\n",
    "\n",
    "# setting train, validation and test sizes as strings as required by sagemaker arguments\n",
    "train_size = 0.6\n",
    "val_size = 0.2\n",
    "test_size = 0.2\n",
    "random_seed = 42 # setting random seed to ensure compatible results over multiple executions\n",
    "\n",
    "# Useful SageMaker variables\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role_arn= sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# Local data paths\n",
    "pipeline_scripts_dir = os.path.join(parent_dir, 'pipeline_scripts')\n",
    "os.makedirs(pipeline_scripts_dir, exist_ok=True)\n",
    "\n",
    "processed_dir = os.path.join(parent_dir, '/data/processed')\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "processed_train_dir = os.path.join(parent_dir, f'{processed_dir}/train')\n",
    "os.makedirs(processed_train_dir, exist_ok=True)\n",
    "\n",
    "processed_validation_dir = os.path.join(parent_dir, f'{processed_dir}/validation')\n",
    "os.makedirs(processed_validation_dir, exist_ok=True)\n",
    "\n",
    "processed_test_dir = os.path.join(parent_dir, f'{processed_dir}/test')\n",
    "os.makedirs(processed_test_dir, exist_ok=True)\n",
    "\n",
    "sm_processed_dir = os.path.join(parent_dir, '/data/sm_processed')\n",
    "os.makedirs(sm_processed_dir, exist_ok=True)\n",
    "\n",
    "sm_processed_train_dir = os.path.join(parent_dir, f'{sm_processed_dir}/train')\n",
    "os.makedirs(sm_processed_train_dir, exist_ok=True)\n",
    "\n",
    "sm_processed_validation_dir = os.path.join(parent_dir, f'{sm_processed_dir}/validation')\n",
    "os.makedirs(sm_processed_validation_dir, exist_ok=True)\n",
    "\n",
    "sm_processed_test_dir = os.path.join(parent_dir, f'{sm_processed_dir}/test')\n",
    "os.makedirs(sm_processed_test_dir, exist_ok=True)\n",
    "\n",
    "# Data paths in S3\n",
    "s3_prefix = 'mlops-workshop'\n",
    "\n",
    "# SageMaker Processing variables\n",
    "processing_instance_type = 'ml.m5.large'\n",
    "processing_instance_count = 1\n",
    "output_path = f's3://{bucket}/{s3_prefix}/data/sm_processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Processing our dataset locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> \n",
    "\t<font size=5>⚠️ <strong> Warning: </strong> PART I is included only to highlight the different steps involved the data preparation stages for machine learning. Doing data processing or model training using Studio notebook instance is not recommended for developing ML projects. The recommeded method is covered in PART II.</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Data**\n",
    "\n",
    "First we'll read our dataset from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>3063.659964</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>2696.878986</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>381981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>3128.891776</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>522583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005</td>\n",
       "      <td>1601.547000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>304132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>2298.760054</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>277414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        2008  3063.659964             2            1.0       1.37   \n",
       "1        1991  2696.878986             3            3.0       0.83   \n",
       "2        2004  3128.891776             6            1.0       1.55   \n",
       "3        2005  1601.547000             4            2.5       0.76   \n",
       "4        1980  2298.760054             4            2.5       1.34   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH  DECK   PRICE  \n",
       "0              2            0     0  490098  \n",
       "1              2            0     0  381981  \n",
       "2              2            1     1  522583  \n",
       "3              3            0     1  304132  \n",
       "4              2            0     1  277414  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "df = pd.read_csv(local_data_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing target to the first column in the DataFrame**\n",
    "\n",
    "Some SageMaker built-in algorithms for CSV training assume that the target variable is in the first column and that the CSV does not have a header record. Even though we'll be bringing our own model, let's do this change so that if we feel like using built-in algorithms, we can easily switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490098</td>\n",
       "      <td>2008</td>\n",
       "      <td>3063.659964</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381981</td>\n",
       "      <td>1991</td>\n",
       "      <td>2696.878986</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>522583</td>\n",
       "      <td>2004</td>\n",
       "      <td>3128.891776</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304132</td>\n",
       "      <td>2005</td>\n",
       "      <td>1601.547000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277414</td>\n",
       "      <td>1980</td>\n",
       "      <td>2298.760054</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRICE  YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0  490098        2008  3063.659964             2            1.0       1.37   \n",
       "1  381981        1991  2696.878986             3            3.0       0.83   \n",
       "2  522583        2004  3128.891776             6            1.0       1.55   \n",
       "3  304132        2005  1601.547000             4            2.5       0.76   \n",
       "4  277414        1980  2298.760054             4            2.5       1.34   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH  DECK  \n",
       "0              2            0     0  \n",
       "1              2            0     0  \n",
       "2              2            1     1  \n",
       "3              3            0     1  \n",
       "4              2            0     1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shift column 'PRICE' to first position\n",
    "first_column = df.pop('PRICE')\n",
    "  \n",
    "# insert column using insert(position,column_name,\n",
    "# first_column) function\n",
    "df.insert(0, 'PRICE', first_column)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting and Scaling data**\n",
    "\n",
    "Now let's process our data for a Machine Learning model.\n",
    "\n",
    "First we will split the data into train, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6000 - 60% of total\n",
      "Val size: 2000 - 20% of total\n",
      "Test size: 2000 - 20% of total\n"
     ]
    }
   ],
   "source": [
    "# splitting data into train, validation, and test sets \n",
    "rest_size = 1.0 - train_size\n",
    "df_train, df_rest = train_test_split(\n",
    "    df,\n",
    "    test_size=rest_size,\n",
    "    train_size=train_size,\n",
    "    random_state=random_seed\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "    df_rest,\n",
    "    test_size=(test_size / rest_size),\n",
    "    train_size=(val_size / rest_size),\n",
    "    random_state=random_seed\n",
    ")\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_val.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "train_perc = int((len(df_train)/len(df)) * 100)\n",
    "\n",
    "#Sanity check\n",
    "print(f\"Training size: {len(df_train)} - {train_perc}% of total\")\n",
    "val_perc = int((len(df_val)/len(df)) * 100)\n",
    "print(f\"Val size: {len(df_val)} - {val_perc}% of total\")\n",
    "test_perc = int((len(df_test)/len(df)) * 100)\n",
    "print(f\"Test size: {len(df_test)} - {test_perc}% of total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we scale the data based on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaling to training data and transforming dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>539597</td>\n",
       "      <td>0.741691</td>\n",
       "      <td>0.395594</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-1.069785</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305947</td>\n",
       "      <td>-0.874597</td>\n",
       "      <td>-0.706179</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>1.167298</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579711</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.773965</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.448235</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469133</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>-0.225607</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>0.488183</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>493177</td>\n",
       "      <td>1.448816</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.390671</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRICE  YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0  539597    0.741691     0.395594      1.394301      -0.002018  -1.069785   \n",
       "1  305947   -0.874597    -0.706179     -1.425317       1.422264   1.167298   \n",
       "2  579711    0.135583     0.773965      1.394301       0.710123   0.448235   \n",
       "3  469133    0.842709    -0.225607     -0.015508      -0.714158   0.488183   \n",
       "4  493177    1.448816     0.021234     -0.015508       0.710123  -0.390671   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH      DECK  \n",
       "0       0.447551    -0.976607 -0.999000  \n",
       "1      -0.435773     1.023953  1.001001  \n",
       "2       1.330875     1.023953 -0.999000  \n",
       "3       1.330875    -0.976607 -0.999000  \n",
       "4      -1.319097     1.023953 -0.999000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling data\n",
    "scaler_data = StandardScaler()\n",
    "    \n",
    "# fit scaler to training dataset\n",
    "print(\"Fitting scaling to training data and transforming dataset...\")\n",
    "df_train_transformed = pd.DataFrame(\n",
    "    scaler_data.fit_transform(df_train), \n",
    "    columns=df_train.columns\n",
    ")\n",
    "df_train_transformed['PRICE'] = df_train['PRICE']\n",
    "df_train_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and apply this scaling to the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming validation dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556055</td>\n",
       "      <td>1.751870</td>\n",
       "      <td>0.260710</td>\n",
       "      <td>-0.720412</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.031139</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153828</td>\n",
       "      <td>-1.480704</td>\n",
       "      <td>-1.937217</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>-1.029837</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>453257</td>\n",
       "      <td>-0.369507</td>\n",
       "      <td>0.522072</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>478332</td>\n",
       "      <td>-0.470525</td>\n",
       "      <td>0.438346</td>\n",
       "      <td>-0.720412</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.670306</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>368541</td>\n",
       "      <td>-0.571543</td>\n",
       "      <td>-0.038212</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>-1.426299</td>\n",
       "      <td>-1.868743</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRICE  YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0  556055    1.751870     0.260710     -0.720412       0.710123  -0.031139   \n",
       "1  153828   -1.480704    -1.937217      0.689397      -0.714158  -1.029837   \n",
       "2  453257   -0.369507     0.522072     -0.015508      -0.002018   0.168600   \n",
       "3  478332   -0.470525     0.438346     -0.720412       0.710123  -0.670306   \n",
       "4  368541   -0.571543    -0.038212      0.689397      -1.426299  -1.868743   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH      DECK  \n",
       "0       0.447551    -0.976607  1.001001  \n",
       "1       0.447551    -0.976607 -0.999000  \n",
       "2      -1.319097    -0.976607 -0.999000  \n",
       "3       1.330875    -0.976607 -0.999000  \n",
       "4      -1.319097     1.023953  1.001001  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply scaler to validation dataset\n",
    "print(\"Transforming validation dataset...\")\n",
    "df_val_transformed = pd.DataFrame(\n",
    "    scaler_data.transform(df_val), \n",
    "    columns=df_val.columns\n",
    ")\n",
    "df_val_transformed['PRICE'] = df_val['PRICE']\n",
    "df_val_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347477</td>\n",
       "      <td>-1.278668</td>\n",
       "      <td>-0.223853</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-0.670306</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305636</td>\n",
       "      <td>-1.581722</td>\n",
       "      <td>-0.447193</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>0.767819</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>446178</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>-0.282516</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.048756</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>503543</td>\n",
       "      <td>0.236601</td>\n",
       "      <td>0.545417</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>0.887662</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310304</td>\n",
       "      <td>-0.470525</td>\n",
       "      <td>-1.126331</td>\n",
       "      <td>0.689397</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.590410</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRICE  YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0  347477   -1.278668    -0.223853     -1.425317      -0.002018  -0.670306   \n",
       "1  305636   -1.581722    -0.447193      0.689397      -0.714158   0.767819   \n",
       "2  446178    0.842709    -0.282516     -0.015508       0.710123   0.048756   \n",
       "3  503543    0.236601     0.545417     -0.015508       1.422264   0.887662   \n",
       "4  310304   -0.470525    -1.126331      0.689397       0.710123  -0.590410   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH      DECK  \n",
       "0       1.330875    -0.976607  1.001001  \n",
       "1      -0.435773    -0.976607  1.001001  \n",
       "2      -0.435773     1.023953  1.001001  \n",
       "3      -1.319097     1.023953  1.001001  \n",
       "4       0.447551    -0.976607  1.001001  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply scaler to test dataset\n",
    "print(\"Transforming test dataset...\")\n",
    "df_test_transformed = pd.DataFrame(\n",
    "    scaler_data.transform(df_test), \n",
    "    columns=df_test.columns\n",
    ")\n",
    "df_test_transformed['PRICE'] = df_test['PRICE']\n",
    "df_test_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Let's go ahead and save our generated dataset so that we can do some preprocessing locally on our notebook's instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save data locally\n",
    "df_train_transformed.to_csv(processed_train_dir+'/train.csv', sep=',', index=False, header=False)\n",
    "df_val_transformed.to_csv(processed_validation_dir+'/validation.csv', sep=',', index=False, header=False)\n",
    "df_test_transformed.to_csv(processed_test_dir+'/test.csv', sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II - SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process large amounts of data, we fortunately will not need to write distributed code oursleves. Instead, we can use [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) which will do all the processing _outside_ of this notebook's resources and will apply our processing script to multiple data files in parallel.\n",
    "    \n",
    "Keep in mind that in a typical SageMaker workflow, notebooks are only used for initial model development activities and can be run on relatively inexpensive and less powerful instances. However, to run similar tasks at scale, data scientists require access to more powerful SageMaker managed compute instances for data preparation, training, and model hosting tasks. \n",
    "\n",
    "SageMaker Processing includes off-the-shelf support for [scikit-learn](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html), [PySpark](https://docs.aws.amazon.com/sagemaker/latest/dg/use-spark-processing-container.html), and [other frameworks](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks.html) like Hugging Face, MXNet, PyTorch, TensorFlow, and XGBoost. You can even a Bring Your Own Container if one our our built-in containers does not suit your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To leverage SageMaker Processing, we'll need our raw data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload raw data to S3\n",
    "raw_data_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/house_pricing.csv', key_prefix=raw_data_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll simply supply our Python processing script with a simple modifications to replace the local path we saved our processed data to with the correct container path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../pipeline_scripts/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pipeline_scripts/preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_parameters():\n",
    "    \"\"\"\n",
    "    Read job parameters\n",
    "    Returns:\n",
    "        (Namespace): read parameters\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_size', type=float, default=0.6)\n",
    "    parser.add_argument('--val_size', type=float, default=0.2)\n",
    "    parser.add_argument('--test_size', type=float, default=0.2)\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    parser.add_argument('--target_col', type=str, default='PRICE')\n",
    "    params, _ = parser.parse_known_args()\n",
    "    return params\n",
    "\n",
    "\n",
    "def change_target_to_first_col(df, target_col):\n",
    "    # shift column 'PRICE' to first position\n",
    "    first_column = df.pop(target_col)\n",
    "  \n",
    "    # insert column using insert(position,column_name,\n",
    "    # first_column) function\n",
    "    df.insert(0, target_col, first_column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_dataset(df, train_size, val_size, test_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation and test samples\n",
    "    Args:\n",
    "        df (pandas.DataFrame): input data\n",
    "        train_size (float): ratio of data to use as training dataset\n",
    "        val_size (float): ratio of data to use as validation dataset\n",
    "        test_size (float): ratio of data to use as test dataset\n",
    "        random_state (int): Pass an int for reproducible output across multiple function calls.\n",
    "    Returns:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "    \"\"\"\n",
    "    if (train_size + val_size + test_size) != 1.0:\n",
    "        raise ValueError(\"train_size, val_size and test_size must sum up to 1.0\")\n",
    "    rest_size = 1 - train_size\n",
    "    df_train, df_rest = train_test_split(\n",
    "        df,\n",
    "        test_size=rest_size,\n",
    "        train_size=train_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_rest,\n",
    "        test_size=(test_size / rest_size),\n",
    "        train_size=(val_size / rest_size),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_val.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    train_perc = int((len(df_train)/len(df)) * 100)\n",
    "    print(f\"Training size: {len(df_train)} - {train_perc}% of total\")\n",
    "    val_perc = int((len(df_val)/len(df)) * 100)\n",
    "    print(f\"Val size: {len(df_val)} - {val_perc}% of total\")\n",
    "    test_perc = int((len(df_test)/len(df)) * 100)\n",
    "    print(f\"Test size: {len(df_test)} - {test_perc}% of total\")\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def scale_dataset(df_train, df_val, df_test, target_col):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler to df_train and apply it to df_val and df_test\n",
    "    Args:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "        target_col (str): target col\n",
    "    Returns:\n",
    "        df_train_transformed (pandas.DataFrame): train data scaled\n",
    "        df_val_transformed (pandas.DataFrame): val data scaled\n",
    "        df_test_transformed (pandas.DataFrame): test data scaled\n",
    "    \"\"\"\n",
    "    scaler_data = StandardScaler()\n",
    "    \n",
    "    # fit scaler to training dataset\n",
    "    print(\"Fitting scaling to training data and transforming dataset...\")\n",
    "    df_train_transformed = pd.DataFrame(\n",
    "        scaler_data.fit_transform(df_train), \n",
    "        columns=df_train.columns\n",
    "    )\n",
    "    df_train_transformed[target_col] = df_train[target_col]\n",
    "    \n",
    "    # apply scaler to validation and test datasets\n",
    "    print(\"Transforming validation and test datasets...\")\n",
    "    df_val_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_val), \n",
    "        columns=df_val.columns\n",
    "    )\n",
    "    df_val_transformed[target_col] = df_val[target_col]\n",
    "    df_test_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_test), \n",
    "        columns=df_test.columns\n",
    "    )\n",
    "    df_test_transformed[target_col] = df_test[target_col]\n",
    "    return df_train_transformed, df_val_transformed, df_test_transformed\n",
    "\n",
    "\n",
    "print(f\"===========================================================\")\n",
    "print(f\"Starting pre-processing\")\n",
    "print(f\"Reading parameters\")\n",
    "\n",
    "# reading job parameters\n",
    "args = read_parameters()\n",
    "print(f\"Parameters read: {args}\")\n",
    "\n",
    "# set input and output paths\n",
    "input_data_path = \"/opt/ml/processing/input/house_pricing.csv\"\n",
    "train_data_path = \"/opt/ml/processing/output/train\"\n",
    "val_data_path = \"/opt/ml/processing/output/validation\"\n",
    "test_data_path = \"/opt/ml/processing/output/test\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(train_data_path)\n",
    "    os.makedirs(val_data_path)\n",
    "    os.makedirs(test_data_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# read data input\n",
    "df = pd.read_csv(input_data_path)\n",
    "\n",
    "# move target to first col\n",
    "df = change_target_to_first_col(df, args.target_col)\n",
    "\n",
    "# split dataset into train, validation and test\n",
    "df_train, df_val, df_test = split_dataset(\n",
    "    df,\n",
    "    train_size=args.train_size,\n",
    "    val_size=args.val_size,\n",
    "    test_size=args.test_size,\n",
    "    random_state=args.random_state\n",
    ")\n",
    "\n",
    "# scale datasets\n",
    "df_train_transformed, df_val_transformed, df_test_transformed = scale_dataset(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    df_test,\n",
    "    args.target_col\n",
    ")\n",
    "\n",
    "print(\"Saving data\")\n",
    "df_train_transformed.to_csv(train_data_path+'/train.csv', sep=',', index=False, header=False)\n",
    "df_val_transformed.to_csv(val_data_path+'/validation.csv', sep=',', index=False, header=False)\n",
    "df_test_transformed.to_csv(test_data_path+'/test.csv', sep=',', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ending pre-processing\")\n",
    "print(f\"===========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using scikit-learn in our processing script, we'll tell SageMaker Processing that we'll need the scikit-learn processor (the container associated with this processor also includes common libraries like `pandas`) which the SageMaker SDK calls `SKLearnProcessor`. This object allows you to specify the instance type to use in the job as well as how many instances you want in your cluster. Although the synthetic housing dataset is quite small, we'll use two instances to showcase how easy it is to spin up a cluster for SageMaker Processing and parallelize your processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='1.0-1',\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run the Processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1513/2663651047.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-2#/processing-jobs/processing-2023-07-13-03-51-14\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name processing-2023-07-13-03-51-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34m===========================================================\u001b[0m\n",
      "\u001b[34mStarting pre-processing\u001b[0m\n",
      "\u001b[34mReading parameters\u001b[0m\n",
      "\u001b[34mParameters read: Namespace(random_state=42, target_col='PRICE', test_size=0.2, train_size=0.6, val_size=0.2)\u001b[0m\n",
      "\u001b[34mTraining size: 6000 - 60% of total\u001b[0m\n",
      "\u001b[34mVal size: 2000 - 20% of total\u001b[0m\n",
      "\u001b[34mTest size: 2000 - 20% of total\u001b[0m\n",
      "\u001b[34mFitting scaling to training data and transforming dataset...\u001b[0m\n",
      "\u001b[34mTransforming validation and test datasets...\u001b[0m\n",
      "\u001b[34mSaving data\u001b[0m\n",
      "\u001b[34mEnding pre-processing\u001b[0m\n",
      "\u001b[34m===========================================================\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "# code=can be a s3 uri for the input script\n",
    "job_name = f\"processing-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(\n",
    "            region, job_name\n",
    "        )\n",
    "    )\n",
    ")\n",
    "sklearn_processor.run(\n",
    "    code='../pipeline_scripts/preprocessing.py',\n",
    "    job_name=job_name,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_s3,\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            destination=f'{output_path}/train',\n",
    "            source='/opt/ml/processing/output/train'\n",
    "        ),          \n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            destination=f'{output_path}/validation',\n",
    "            source='/opt/ml/processing/output/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            destination=f'{output_path}/test',\n",
    "            source='/opt/ml/processing/output/test'\n",
    "        )\n",
    "    ],\n",
    "    # notice that all arguments passed to a SageMaker processing job should be strings as they are transformed to command line parameters.\n",
    "    # Your read_parameters function will handle the data types for your code \n",
    "    arguments=[\n",
    "        \"--train_size\", str(train_size),\n",
    "        \"--val_size\", str(val_size),\n",
    "        \"--test_size\", str(test_size),\n",
    "        \"--random_state\", str(random_seed)\n",
    "    ]\n",
    ")\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the SageMaker Processing job has finished, it has output the processed data in S3. Let's download that data locally and ensure it's what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-240487350066/mlops-workshop/data/sm_processed/validation/validation.csv to ../data/sm_processed/validation/validation.csv\n",
      "download: s3://sagemaker-us-east-2-240487350066/mlops-workshop/data/sm_processed/test/test.csv to ../data/sm_processed/test/test.csv\n",
      "download: s3://sagemaker-us-east-2-240487350066/mlops-workshop/data/sm_processed/train/train.csv to ../data/sm_processed/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Download processed data from S3 to local storage\n",
    "!aws s3 cp {output_path} ../data/sm_processed/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sm_processed/train/train.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sm_processed_dir}/train/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>539597</td>\n",
       "      <td>0.741691</td>\n",
       "      <td>0.395594</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-1.069785</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305947</td>\n",
       "      <td>-0.874597</td>\n",
       "      <td>-0.706179</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>1.167298</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579711</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.773965</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.448235</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469133</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>-0.225607</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>0.488183</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>493177</td>\n",
       "      <td>1.448816</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.390671</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PRICE  YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0  539597    0.741691     0.395594      1.394301      -0.002018  -1.069785   \n",
       "1  305947   -0.874597    -0.706179     -1.425317       1.422264   1.167298   \n",
       "2  579711    0.135583     0.773965      1.394301       0.710123   0.448235   \n",
       "3  469133    0.842709    -0.225607     -0.015508      -0.714158   0.488183   \n",
       "4  493177    1.448816     0.021234     -0.015508       0.710123  -0.390671   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH      DECK  \n",
       "0       0.447551    -0.976607 -0.999000  \n",
       "1      -0.435773     1.023953  1.001001  \n",
       "2       1.330875     1.023953 -0.999000  \n",
       "3       1.330875    -0.976607 -0.999000  \n",
       "4      -1.319097     1.023953 -0.999000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They match!\n"
     ]
    }
   ],
   "source": [
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "sm_processed_df_train_transformed = pd.read_csv(f'../{sm_processed_dir}/train/train.csv', sep=',', header=None)\n",
    "sm_processed_df_train_transformed.columns = df_train_transformed.columns\n",
    "try:\n",
    "    assert_frame_equal(df_train_transformed, sm_processed_df_train_transformed)\n",
    "    print(\"They match!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PART III - SageMaker Feature Store\n",
    "    \n",
    "Features are inputs to ML models used during training and inference. Features are used repeatedly by multiple teams and feature quality is critical to ensure a highly accurate model. Also, when features used to train models offline in batch are made available for real-time inference, it’s hard to keep the two feature stores synchronized. [SageMaker Feature Store](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) provides a secured and unified store for feature use across the ML lifecycle. \n",
    "\n",
    "Let's now exchange the storage of our processed data from s3 to SageMaker Feature Store.\n",
    "\n",
    "![Notebook1](./images/notebook-1fs.png)\n",
    "\n",
    "**Feature Groups**\n",
    "\n",
    "First let's define some feature groups for train, validation and test datasets and a s3 bucket prefix to store your feature store results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> \n",
    "\t<font size=5>⚠️ <strong> Alert: </strong> Although the data in this notebook has been first split into train, validation and test set and Feature Groups created for each set, this is not the recommended practice in production. You will want to create one Feature Group and retrieve related features per project, then split it into the required set for model training.</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_str = '-' + time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "train_feature_group_name = \"fs-train-\"+time_str\n",
    "validation_feature_group_name = \"fs-validation-\"+time_str\n",
    "test_feature_group_name = \"fs-test-\"+time_str\n",
    "bucket_prefix = \"mlops-workshop/feature-store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify our Python processing script to include the creation of feature groups for the train, validation and test datasets and the injection of data to the feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../pipeline_scripts/preprocessing_with_fs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pipeline_scripts/preprocessing_with_fs.py\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker'])\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "def read_parameters():\n",
    "    \"\"\"\n",
    "    Read job parameters\n",
    "    Returns:\n",
    "        (Namespace): read parameters\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_size', type=float, default=0.6)\n",
    "    parser.add_argument('--val_size', type=float, default=0.2)\n",
    "    parser.add_argument('--test_size', type=float, default=0.2)\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    parser.add_argument('--train_feature_group_name', type=str, default='fs-train')\n",
    "    parser.add_argument('--validation_feature_group_name', type=str, default='fs-validation')\n",
    "    parser.add_argument('--test_feature_group_name', type=str, default='fs-test')\n",
    "    parser.add_argument('--bucket_prefix', type=str, default='mlops-workshop/feature-store')\n",
    "    parser.add_argument('--target_col', type=str, default='PRICE')\n",
    "    parser.add_argument('--region', type=str)\n",
    "    parser.add_argument('--role_arn', type=str)\n",
    "    params, _ = parser.parse_known_args()\n",
    "    return params\n",
    "\n",
    "\n",
    "def change_target_to_first_col(df, target_col):\n",
    "    # shift column 'PRICE' to first position\n",
    "    first_column = df.pop(target_col)\n",
    "  \n",
    "    # insert column using insert(position,column_name,\n",
    "    # first_column) function\n",
    "    df.insert(0, target_col, first_column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_dataset(df, train_size, val_size, test_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation and test samples\n",
    "    Args:\n",
    "        df (pandas.DataFrame): input data\n",
    "        train_size (float): ratio of data to use as training dataset\n",
    "        val_size (float): ratio of data to use as validation dataset\n",
    "        test_size (float): ratio of data to use as test dataset\n",
    "        random_state (int): Pass an int for reproducible output across multiple function calls.\n",
    "    Returns:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "    \"\"\"\n",
    "    if (train_size + val_size + test_size) != 1.0:\n",
    "        raise ValueError(\"train_size, val_size and test_size must sum up to 1.0\")\n",
    "    rest_size = 1 - train_size\n",
    "    df_train, df_rest = train_test_split(\n",
    "        df,\n",
    "        test_size=rest_size,\n",
    "        train_size=train_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_rest,\n",
    "        test_size=(test_size / rest_size),\n",
    "        train_size=(val_size / rest_size),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_val.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    train_perc = int((len(df_train)/len(df)) * 100)\n",
    "    print(f\"Training size: {len(df_train)} - {train_perc}% of total\")\n",
    "    val_perc = int((len(df_val)/len(df)) * 100)\n",
    "    print(f\"Val size: {len(df_val)} - {val_perc}% of total\")\n",
    "    test_perc = int((len(df_test)/len(df)) * 100)\n",
    "    print(f\"Test size: {len(df_test)} - {test_perc}% of total\")\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def scale_dataset(df_train, df_val, df_test, target_col):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler to df_train and apply it to df_val and df_test\n",
    "    Args:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "        target_col (str): target col\n",
    "    Returns:\n",
    "        df_train_transformed (pandas.DataFrame): train data scaled\n",
    "        df_val_transformed (pandas.DataFrame): val data scaled\n",
    "        df_test_transformed (pandas.DataFrame): test data scaled\n",
    "    \"\"\"\n",
    "    scaler_data = StandardScaler()\n",
    "    \n",
    "    # fit scaler to training dataset\n",
    "    print(\"Fitting scaling to training data and transforming dataset...\")\n",
    "    df_train_transformed = pd.DataFrame(\n",
    "        scaler_data.fit_transform(df_train), \n",
    "        columns=df_train.columns\n",
    "    )\n",
    "    df_train_transformed[target_col] = df_train[target_col]\n",
    "    \n",
    "    # apply scaler to validation and test datasets\n",
    "    print(\"Transforming validation and test datasets...\")\n",
    "    df_val_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_val), \n",
    "        columns=df_val.columns\n",
    "    )\n",
    "    df_val_transformed[target_col] = df_val[target_col]\n",
    "    df_test_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_test), \n",
    "        columns=df_test.columns\n",
    "    )\n",
    "    df_test_transformed[target_col] = df_test[target_col]\n",
    "    return df_train_transformed, df_val_transformed, df_test_transformed\n",
    "\n",
    "\n",
    "def prepare_df_for_feature_store(df, data_type):\n",
    "    \"\"\"\n",
    "    Add event time and record id to df in order to store it in SageMaker Feature Store\n",
    "    Args:\n",
    "        df (pandas.DataFrame): data to be prepared\n",
    "        data_type (str): train/validation or test\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): dataframe with event time and record id\n",
    "    \"\"\"\n",
    "    print(f'Preparing {data_type} data for Feature Store..')\n",
    "    current_time_sec = int(round(time.time()))\n",
    "    # create event time\n",
    "    df['event_time'] = pd.Series([current_time_sec]*len(df), dtype=\"float64\")\n",
    "    # create record id from index\n",
    "    df['record_id'] = df.reset_index().index\n",
    "    return df\n",
    "    \n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    \"\"\"\n",
    "    Function that waits for feature group to be created in SageMaker Feature Store\n",
    "    Args:\n",
    "        feature_group (sagemaker.feature_store.feature_group.FeatureGroup): Feature Group\n",
    "    \"\"\"\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        print(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    print(f'FeatureGroup {feature_group.name} was successfully created.')\n",
    "    \n",
    "\n",
    "def create_feature_group(feature_group_name, sagemaker_session, df, prefix, role_arn):\n",
    "    \"\"\"\n",
    "    Create Feature Store Group\n",
    "    Args:\n",
    "        feature_group_name (str): Feature Store Group Name\n",
    "        sagemaker_session (sagemaker.session.Session): sagemaker session\n",
    "        df (pandas.DataFrame): dataframe to injest used to create features definition\n",
    "        prefix (str): geature group prefix (train/validation or test)\n",
    "        role_arn (str): role arn to create feature store\n",
    "    Returns:\n",
    "        fs_group (sagemaker.feature_store.feature_group.FeatureGroup): Feature Group\n",
    "    \"\"\"\n",
    "    fs_group = FeatureGroup(\n",
    "        name=feature_group_name, \n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    fs_group.load_feature_definitions(data_frame=df)\n",
    "    default_bucket = sagemaker_session.default_bucket()\n",
    "    print(f'Creating feature group: {fs_group.name} ...')\n",
    "    fs_group.create(\n",
    "        s3_uri=f's3://{default_bucket}/{prefix}', \n",
    "        record_identifier_name='record_id', \n",
    "        event_time_feature_name='event_time', \n",
    "        role_arn=role_arn, \n",
    "        enable_online_store=True\n",
    "    )\n",
    "    wait_for_feature_group_creation_complete(fs_group)\n",
    "    return fs_group\n",
    "\n",
    "\n",
    "def ingest_features(fs_group, df):\n",
    "    \"\"\"\n",
    "    Ingest features to Feature Store Group\n",
    "    Args:\n",
    "        fs_group (sagemaker.feature_store.feature_group.FeatureGroup): Feature Group\n",
    "        df (pandas.DataFrame): dataframe to injest\n",
    "    \"\"\"\n",
    "    print(f'Ingesting data into feature group: {fs_group.name} ...')\n",
    "    fs_group.ingest(data_frame=df, max_processes=3, wait=True)\n",
    "    print(f'{len(df)} records ingested into feature group: {fs_group.name}')\n",
    "    return\n",
    "\n",
    "\n",
    "print(f\"===========================================================\")\n",
    "print(f\"Starting pre-processing\")\n",
    "print(f\"Reading parameters\")\n",
    "\n",
    "# reading job parameters\n",
    "args = read_parameters()\n",
    "print(f\"Parameters read: {args}\")\n",
    "sagemaker_session = sagemaker.Session(boto3.Session(region_name=args.region))\n",
    "\n",
    "# set input path\n",
    "input_data_path = \"/opt/ml/processing/input/house_pricing.csv\"\n",
    "\n",
    "# read data input\n",
    "df = pd.read_csv(input_data_path)\n",
    "\n",
    "# move target to first col\n",
    "df = change_target_to_first_col(df, args.target_col)\n",
    "\n",
    "# split dataset into train, validation and test\n",
    "df_train, df_val, df_test = split_dataset(\n",
    "    df,\n",
    "    train_size=args.train_size,\n",
    "    val_size=args.val_size,\n",
    "    test_size=args.test_size,\n",
    "    random_state=args.random_state\n",
    ")\n",
    "\n",
    "# scale datasets\n",
    "df_train_transformed, df_val_transformed, df_test_transformed = scale_dataset(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    df_test,\n",
    "    args.target_col\n",
    ")\n",
    "\n",
    "# prepare datasets for Feature Store\n",
    "df_train_transformed_fs = prepare_df_for_feature_store(df_train_transformed, 'train')\n",
    "df_val_transformed_fs = prepare_df_for_feature_store(df_val_transformed, 'validation')\n",
    "df_test_transformed_fs = prepare_df_for_feature_store(df_test_transformed, 'test')\n",
    "\n",
    "# injest datasets to Feature Store\n",
    "fs_group_train = create_feature_group(\n",
    "    args.train_feature_group_name, \n",
    "    sagemaker_session, \n",
    "    df_train_transformed_fs, \n",
    "    args.bucket_prefix+'/train',\n",
    "    args.role_arn\n",
    ")\n",
    "ingest_features(fs_group_train, df_train_transformed_fs)\n",
    "\n",
    "fs_group_validation = create_feature_group(\n",
    "    args.validation_feature_group_name, \n",
    "    sagemaker_session, \n",
    "    df_val_transformed_fs, \n",
    "    args.bucket_prefix+'/validation',\n",
    "    args.role_arn\n",
    ")\n",
    "ingest_features(fs_group_validation, df_val_transformed_fs)\n",
    "\n",
    "fs_group_test = create_feature_group(\n",
    "    args.test_feature_group_name, \n",
    "    sagemaker_session, \n",
    "    df_test_transformed_fs, \n",
    "    args.bucket_prefix+'/test',\n",
    "    args.role_arn\n",
    ")\n",
    "ingest_features(fs_group_test, df_test_transformed_fs)\n",
    "\n",
    "print(f\"Ending pre-processing\")\n",
    "print(f\"===========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run the Processing job with the feature store code included on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-2#/processing-jobs/processing-with-fs-2023-07-13-03-56-09\">Feature Store Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name processing-with-fs-2023-07-13-03-56-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34mCollecting sagemaker\n",
      "  Downloading sagemaker-2.171.0.tar.gz (853 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 853.5/853.5 kB 32.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting attrs<24,>=23.1.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting boto3<2.0,>=1.26.131\n",
      "  Downloading boto3-1.28.2-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.7/135.7 kB 27.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 12.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /miniconda3/lib/python3.8/site-packages (from sagemaker) (1.24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /miniconda3/lib/python3.8/site-packages (from sagemaker) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata<5.0,>=1.4.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=20.0\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.8/site-packages (from sagemaker) (1.1.3)\u001b[0m\n",
      "\u001b[34mCollecting pathos\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting PyYAML==6.0\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 701.2/701.2 kB 44.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting jsonschema\n",
      "  Downloading jsonschema-4.18.2-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 18.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting platformdirs\n",
      "  Downloading platformdirs-3.8.1-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting tblib==1.7.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /miniconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /miniconda3/lib/python3.8/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.32.0,>=1.31.2\n",
      "  Downloading botocore-1.31.2-py3-none-any.whl (11.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/11.0 MB 92.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.16.1-py3-none-any.whl (7.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.8/site-packages (from google-pasta->sagemaker) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.6.1-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.8.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 77.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting referencing>=0.28.4\n",
      "  Downloading referencing-0.29.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting pkgutil-resolve-name>=1.3.10\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources>=1.4.0\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /miniconda3/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas->sagemaker) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting ppft>=1.7.6.6\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 11.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pox>=0.3.2\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill>=0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.8/site-packages (from botocore<1.32.0,>=1.31.2->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.171.0-py2.py3-none-any.whl size=1160876 sha256=e3b9c35fba5e3c3b66b15d4a34b3275f4521ff5cb4e88660d2694e4dd0f28008\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/48/19/e09c37bdb79f2dc3f3d1a28e5a55a0d89e4a7f012aa72c4e23\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, tblib, smdebug_rulesconfig, rpds-py, PyYAML, ppft, pox, platformdirs, pkgutil-resolve-name, packaging, google-pasta, dill, contextlib2, cloudpickle, attrs, schema, referencing, multiprocess, importlib-resources, importlib-metadata, botocore, pathos, jsonschema-specifications, jsonschema, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.18\n",
      "    Uninstalling botocore-1.27.18:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled botocore-1.27.18\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.17\n",
      "    Uninstalling boto3-1.24.17:\n",
      "      Successfully uninstalled boto3-1.24.17\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires boto3==1.24.17, but you have boto3 1.28.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires botocore==1.27.18, but you have botocore 1.31.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyYAML-6.0 attrs-23.1.0 boto3-1.28.2 botocore-1.31.2 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.6 google-pasta-0.2.0 importlib-metadata-4.13.0 importlib-resources-6.0.0 jsonschema-4.18.2 jsonschema-specifications-2023.6.1 multiprocess-0.70.14 packaging-23.1 pathos-0.3.0 pkgutil-resolve-name-1.3.10 platformdirs-3.8.1 pox-0.3.2 ppft-1.7.6.6 referencing-0.29.1 rpds-py-0.8.10 sagemaker-2.171.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tblib-1.7.0 zipp-3.16.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0.1 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m===========================================================\u001b[0m\n",
      "\u001b[34mStarting pre-processing\u001b[0m\n",
      "\u001b[34mReading parameters\u001b[0m\n",
      "\u001b[34mParameters read: Namespace(bucket_prefix='mlops-workshop/feature-store', random_state=42, region='us-east-2', role_arn='arn:aws:iam::240487350066:role/service-role/AmazonSageMaker-ExecutionRole-20220329T222327', target_col='PRICE', test_feature_group_name='fs-test--2023-07-13-03-56-09', test_size=0.2, train_feature_group_name='fs-train--2023-07-13-03-56-09', train_size=0.6, val_size=0.2, validation_feature_group_name='fs-validation--2023-07-13-03-56-09')\u001b[0m\n",
      "\u001b[34mTraining size: 6000 - 60% of total\u001b[0m\n",
      "\u001b[34mVal size: 2000 - 20% of total\u001b[0m\n",
      "\u001b[34mTest size: 2000 - 20% of total\u001b[0m\n",
      "\u001b[34mFitting scaling to training data and transforming dataset...\u001b[0m\n",
      "\u001b[34mTransforming validation and test datasets...\u001b[0m\n",
      "\u001b[34mPreparing train data for Feature Store..\u001b[0m\n",
      "\u001b[34mPreparing validation data for Feature Store..\u001b[0m\n",
      "\u001b[34mPreparing test data for Feature Store..\u001b[0m\n",
      "\u001b[34mCreating feature group: fs-train--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34mInitial status: Creating\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-train--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-train--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-train--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-train--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mFeatureGroup fs-train--2023-07-13-03-56-09 was successfully created.\u001b[0m\n",
      "\u001b[34mIngesting data into feature group: fs-train--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34m6000 records ingested into feature group: fs-train--2023-07-13-03-56-09\u001b[0m\n",
      "\u001b[34mCreating feature group: fs-validation--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34mInitial status: Creating\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-validation--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-validation--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-validation--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-validation--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mFeatureGroup fs-validation--2023-07-13-03-56-09 was successfully created.\u001b[0m\n",
      "\u001b[34mIngesting data into feature group: fs-validation--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34m2000 records ingested into feature group: fs-validation--2023-07-13-03-56-09\u001b[0m\n",
      "\u001b[34mCreating feature group: fs-test--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34mInitial status: Creating\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-test--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-test--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mWaiting for feature group: fs-test--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\n",
      "\u001b[34mWaiting for feature group: fs-test--2023-07-13-03-56-09 to be created ...\u001b[0m\n",
      "\u001b[34mFeatureGroup fs-test--2023-07-13-03-56-09 was successfully created.\u001b[0m\n",
      "\u001b[34mIngesting data into feature group: fs-test--2023-07-13-03-56-09 ...\u001b[0m\n",
      "\u001b[34m2000 records ingested into feature group: fs-test--2023-07-13-03-56-09\u001b[0m\n",
      "\u001b[34mEnding pre-processing\u001b[0m\n",
      "\u001b[34m===========================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# code=can be a s3 uri for the input script\n",
    "job_name = f\"processing-with-fs-{strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Feature Store Processing Job</a></b>'.format(\n",
    "            region, job_name\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='../pipeline_scripts/preprocessing_with_fs.py',\n",
    "    job_name=job_name,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_s3,\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    # notice that all arguments passed to a SageMaker processing job should be strings as they are transformed to command line parameters.\n",
    "    # Your read_parameters function will handle the data types for your code \n",
    "    arguments=[\n",
    "        \"--train_size\", str(train_size),\n",
    "        \"--val_size\", str(val_size),\n",
    "        \"--test_size\", str(test_size),\n",
    "        \"--random_state\", str(random_seed),\n",
    "        \"--train_feature_group_name\", train_feature_group_name,\n",
    "        \"--validation_feature_group_name\", validation_feature_group_name,\n",
    "        \"--test_feature_group_name\", test_feature_group_name,\n",
    "        \"--bucket_prefix\", bucket_prefix,\n",
    "        \"--role_arn\", role_arn,\n",
    "        \"--region\", region\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the features from feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs_group_train = FeatureGroup(name=train_feature_group_name, sagemaker_session=sess)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_query = fs_group_train.athena_query()\n",
    "train_table = train_query.table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's display the data in a Dataframe:\n",
    "\n",
    "**Important**: If your Dataframe is empty, you should wait a bit before training again. It takes a while for Feature Store data to be available in Athena Queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query 62bf9719-fa8d-4635-a75b-d0f2e238e456 is being executed.\n",
      "INFO:sagemaker:Query 62bf9719-fa8d-4635-a75b-d0f2e238e456 successfully executed.\n",
      "INFO:sagemaker:Query 0b1b7e61-aa4d-4d88-aba1-e64dcfab7d2a is being executed.\n",
      "INFO:sagemaker:Query 0b1b7e61-aa4d-4d88-aba1-e64dcfab7d2a successfully executed.\n",
      "INFO:sagemaker:Query 41607c92-8417-4c62-ab89-b7c8688cf9e0 is being executed.\n",
      "INFO:sagemaker:Query 41607c92-8417-4c62-ab89-b7c8688cf9e0 successfully executed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year_built</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>num_bedrooms</th>\n",
       "      <th>num_bathrooms</th>\n",
       "      <th>lot_acres</th>\n",
       "      <th>garage_spaces</th>\n",
       "      <th>front_porch</th>\n",
       "      <th>deck</th>\n",
       "      <th>event_time</th>\n",
       "      <th>record_id</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>539597</td>\n",
       "      <td>0.741691</td>\n",
       "      <td>0.395594</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-1.069785</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-13 04:06:36.730</td>\n",
       "      <td>2023-07-13 04:00:57.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305947</td>\n",
       "      <td>-0.874597</td>\n",
       "      <td>-0.706179</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>1.167298</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-13 04:06:34.971</td>\n",
       "      <td>2023-07-13 04:00:58.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579711</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.773965</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.448235</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-13 04:06:35.293</td>\n",
       "      <td>2023-07-13 04:00:58.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469133</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>-0.225607</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>0.488183</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07-13 04:06:36.748</td>\n",
       "      <td>2023-07-13 04:00:58.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>493177</td>\n",
       "      <td>1.448816</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.390671</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-07-13 04:06:26.849</td>\n",
       "      <td>2023-07-13 04:00:58.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>284703</td>\n",
       "      <td>-1.581722</td>\n",
       "      <td>-1.167864</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>1.806464</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>5995</td>\n",
       "      <td>2023-07-13 04:06:35.084</td>\n",
       "      <td>2023-07-13 04:01:17.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>361622</td>\n",
       "      <td>-0.773579</td>\n",
       "      <td>-0.427916</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.870045</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>5996</td>\n",
       "      <td>2023-07-13 04:06:36.804</td>\n",
       "      <td>2023-07-13 04:01:17.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>253485</td>\n",
       "      <td>-0.268489</td>\n",
       "      <td>-1.481280</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-1.426299</td>\n",
       "      <td>-0.590410</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>5997</td>\n",
       "      <td>2023-07-13 04:06:26.849</td>\n",
       "      <td>2023-07-13 04:01:17.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>469226</td>\n",
       "      <td>0.438637</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>2.445631</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>5998</td>\n",
       "      <td>2023-07-13 04:06:36.804</td>\n",
       "      <td>2023-07-13 04:01:17.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>227847</td>\n",
       "      <td>-3.096992</td>\n",
       "      <td>-0.461088</td>\n",
       "      <td>-0.720412</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-0.230879</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.689221e+09</td>\n",
       "      <td>5999</td>\n",
       "      <td>2023-07-13 04:06:34.986</td>\n",
       "      <td>2023-07-13 04:01:17.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  year_built  square_feet  num_bedrooms  num_bathrooms  lot_acres  \\\n",
       "0     539597    0.741691     0.395594      1.394301      -0.002018  -1.069785   \n",
       "1     305947   -0.874597    -0.706179     -1.425317       1.422264   1.167298   \n",
       "2     579711    0.135583     0.773965      1.394301       0.710123   0.448235   \n",
       "3     469133    0.842709    -0.225607     -0.015508      -0.714158   0.488183   \n",
       "4     493177    1.448816     0.021234     -0.015508       0.710123  -0.390671   \n",
       "...      ...         ...          ...           ...            ...        ...   \n",
       "5995  284703   -1.581722    -1.167864      1.394301       0.710123   1.806464   \n",
       "5996  361622   -0.773579    -0.427916     -0.015508       0.710123  -0.870045   \n",
       "5997  253485   -0.268489    -1.481280      1.394301      -1.426299  -0.590410   \n",
       "5998  469226    0.438637     0.002270     -1.425317       1.422264   2.445631   \n",
       "5999  227847   -3.096992    -0.461088     -0.720412      -0.002018  -0.230879   \n",
       "\n",
       "      garage_spaces  front_porch      deck    event_time  record_id  \\\n",
       "0          0.447551    -0.976607 -0.999000  1.689221e+09          0   \n",
       "1         -0.435773     1.023953  1.001001  1.689221e+09          1   \n",
       "2          1.330875     1.023953 -0.999000  1.689221e+09          2   \n",
       "3          1.330875    -0.976607 -0.999000  1.689221e+09          3   \n",
       "4         -1.319097     1.023953 -0.999000  1.689221e+09          4   \n",
       "...             ...          ...       ...           ...        ...   \n",
       "5995       1.330875    -0.976607 -0.999000  1.689221e+09       5995   \n",
       "5996       0.447551    -0.976607  1.001001  1.689221e+09       5996   \n",
       "5997      -0.435773     1.023953  1.001001  1.689221e+09       5997   \n",
       "5998       0.447551     1.023953  1.001001  1.689221e+09       5998   \n",
       "5999       0.447551    -0.976607  1.001001  1.689221e+09       5999   \n",
       "\n",
       "                   write_time      api_invocation_time  is_deleted  \n",
       "0     2023-07-13 04:06:36.730  2023-07-13 04:00:57.000       False  \n",
       "1     2023-07-13 04:06:34.971  2023-07-13 04:00:58.000       False  \n",
       "2     2023-07-13 04:06:35.293  2023-07-13 04:00:58.000       False  \n",
       "3     2023-07-13 04:06:36.748  2023-07-13 04:00:58.000       False  \n",
       "4     2023-07-13 04:06:26.849  2023-07-13 04:00:58.000       False  \n",
       "...                       ...                      ...         ...  \n",
       "5995  2023-07-13 04:06:35.084  2023-07-13 04:01:17.000       False  \n",
       "5996  2023-07-13 04:06:36.804  2023-07-13 04:01:17.000       False  \n",
       "5997  2023-07-13 04:06:26.849  2023-07-13 04:01:17.000       False  \n",
       "5998  2023-07-13 04:06:36.804  2023-07-13 04:01:17.000       False  \n",
       "5999  2023-07-13 04:06:34.986  2023-07-13 04:01:17.000       False  \n",
       "\n",
       "[6000 rows x 14 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_df = pd.DataFrame()\n",
    "while len(fs_df) == 0:\n",
    "    if len(fs_df.columns) > 0:\n",
    "        time.sleep(120)\n",
    "    query_string = f'SELECT * FROM \"sagemaker_featurestore\".\"{train_table}\" ORDER BY record_id'\n",
    "    query_results= 'sagemaker-featurestore'\n",
    "    output_location = f's3://{bucket}/{query_results}/query_results/'\n",
    "    train_query.run(query_string=query_string, output_location=output_location)\n",
    "    train_query.wait()\n",
    "    fs_df = train_query.as_dataframe()\n",
    "fs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapting the Athenas Query**\n",
    "\n",
    "Notice that the data before includes extra information that allows us to audit and monitor the data from Feature Store. \n",
    "\n",
    "You can change your data to include only the data necessary for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query 50384c31-741e-4e2f-89b9-14bb85968987 is being executed.\n",
      "INFO:sagemaker:Query 50384c31-741e-4e2f-89b9-14bb85968987 successfully executed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year_built</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>num_bedrooms</th>\n",
       "      <th>num_bathrooms</th>\n",
       "      <th>lot_acres</th>\n",
       "      <th>garage_spaces</th>\n",
       "      <th>front_porch</th>\n",
       "      <th>deck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>539597</td>\n",
       "      <td>0.741691</td>\n",
       "      <td>0.395594</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-1.069785</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305947</td>\n",
       "      <td>-0.874597</td>\n",
       "      <td>-0.706179</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>1.167298</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579711</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.773965</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.448235</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469133</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>-0.225607</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>-0.714158</td>\n",
       "      <td>0.488183</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>493177</td>\n",
       "      <td>1.448816</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.390671</td>\n",
       "      <td>-1.319097</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>284703</td>\n",
       "      <td>-1.581722</td>\n",
       "      <td>-1.167864</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>1.806464</td>\n",
       "      <td>1.330875</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>-0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>361622</td>\n",
       "      <td>-0.773579</td>\n",
       "      <td>-0.427916</td>\n",
       "      <td>-0.015508</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>-0.870045</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>253485</td>\n",
       "      <td>-0.268489</td>\n",
       "      <td>-1.481280</td>\n",
       "      <td>1.394301</td>\n",
       "      <td>-1.426299</td>\n",
       "      <td>-0.590410</td>\n",
       "      <td>-0.435773</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>469226</td>\n",
       "      <td>0.438637</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>-1.425317</td>\n",
       "      <td>1.422264</td>\n",
       "      <td>2.445631</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>1.023953</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>227847</td>\n",
       "      <td>-3.096992</td>\n",
       "      <td>-0.461088</td>\n",
       "      <td>-0.720412</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-0.230879</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>-0.976607</td>\n",
       "      <td>1.001001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  year_built  square_feet  num_bedrooms  num_bathrooms  lot_acres  \\\n",
       "0     539597    0.741691     0.395594      1.394301      -0.002018  -1.069785   \n",
       "1     305947   -0.874597    -0.706179     -1.425317       1.422264   1.167298   \n",
       "2     579711    0.135583     0.773965      1.394301       0.710123   0.448235   \n",
       "3     469133    0.842709    -0.225607     -0.015508      -0.714158   0.488183   \n",
       "4     493177    1.448816     0.021234     -0.015508       0.710123  -0.390671   \n",
       "...      ...         ...          ...           ...            ...        ...   \n",
       "5995  284703   -1.581722    -1.167864      1.394301       0.710123   1.806464   \n",
       "5996  361622   -0.773579    -0.427916     -0.015508       0.710123  -0.870045   \n",
       "5997  253485   -0.268489    -1.481280      1.394301      -1.426299  -0.590410   \n",
       "5998  469226    0.438637     0.002270     -1.425317       1.422264   2.445631   \n",
       "5999  227847   -3.096992    -0.461088     -0.720412      -0.002018  -0.230879   \n",
       "\n",
       "      garage_spaces  front_porch      deck  \n",
       "0          0.447551    -0.976607 -0.999000  \n",
       "1         -0.435773     1.023953  1.001001  \n",
       "2          1.330875     1.023953 -0.999000  \n",
       "3          1.330875    -0.976607 -0.999000  \n",
       "4         -1.319097     1.023953 -0.999000  \n",
       "...             ...          ...       ...  \n",
       "5995       1.330875    -0.976607 -0.999000  \n",
       "5996       0.447551    -0.976607  1.001001  \n",
       "5997      -0.435773     1.023953  1.001001  \n",
       "5998       0.447551     1.023953  1.001001  \n",
       "5999       0.447551    -0.976607  1.001001  \n",
       "\n",
       "[6000 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_select = 'price,year_built,square_feet,num_bedrooms,num_bathrooms,lot_acres,garage_spaces,front_porch,deck'\n",
    "query_string = f'SELECT {features_to_select} FROM \"sagemaker_featurestore\".\"{train_table}\" ORDER BY record_id'\n",
    "query_results= 'sagemaker-featurestore'\n",
    "output_location = f's3://{bucket}/{query_results}/query_results/'\n",
    "train_query.run(query_string=query_string, output_location=output_location)\n",
    "train_query.wait()\n",
    "fs_df = train_query.as_dataframe()\n",
    "fs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "For more in-depth SageMaker Feature Store examples and trainings, please refer to \n",
    "\n",
    "[SageMaker Feature Store end-to-end workshop](https://github.com/aws-samples/amazon-sagemaker-feature-store-end-to-end-workshop/tree/main)\n",
    "\n",
    "[Using Amazon SageMaker Feature Store with streaming feature aggregation](https://github.com/aws-samples/amazon-sagemaker-feature-store-streaming-aggregation/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
